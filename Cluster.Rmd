```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = F)
```

```{r load.packages, include = FALSE}
library(papaja)
library(tidyverse)

rm(list = ls())
```

```{r data}
load("data/base data.rdata")

vars <- c("LSTATE", "LEANM", "SCHNAM", "DID", "SID", "DSID", "n", "pTotfrl",
          "Urban", "Suburban", "ToRu",
          "pELL", "pED", "pELA", "pMath", "pMin", "MEDINC", "MEDINC_SD")

# Covarites 
subs_f_vars <- c("n", "pTotfrl", "urbanicity",
                "pELL", "pED", "pELA", "pMath", "pMin", "MEDINC")

subs_ov_vars <- c("n", "pTotfrl", "urbanicity",
                "pELL", "pED", "pELA", "pMath", "pMin")

# School Level Data
df.sch <- df[,vars]


# District Level Data About students
df.dist <- list(.vars = lst("n", vars[8:17], "m"),
     .funs = lst(mean, funs(weighted.mean(., w = n)), mean)) %>%
  pmap(~ df.sch %>%
         mutate(m = n()) %>%
         group_by(DID) %>%
         summarise_at(.x, .y)) %>%
  reduce(inner_join, by = "DID")


```

## SUBS

Stratification using balanced sampling (SUBS) was performed prior to simulation because the group of schools in each strata would be static across conditions except where the balancing model is manipulated. The set of covariates in both the full model (SUBS-F) and the omitted variable model (SUBS-OV) include binary indicator variables

```{r SUBS_Funcs}
library(parallel)
library(fpc)
library(cluster)
library(tidyverse)


run_clusters <- function(K, distance) {

  # # Calculate the number of cores
  # no_cores <- min(K, detectCores() - 7)
  # 
  # # Initiate cluster
  # cl <- makeCluster(no_cores)
  # 
  # clusters <- parLapply(cl, 1:K, function(x, y) kmeans(x = y, centers = x), distance)
  # 
  # stopCluster(cl)
  # 
  # # clusters <- bind_cols(clusters)
  # #
  # # names(classign) <- paste("k", 1:K, sep = "")
  
  clusters <- list()
  
  for(k in 1:K) {
    clusters <- append(clusters, list(kmeans(x = distance, centers = k)))
  }

  return(clusters)
}



get_CH <- function(K, distance, clusters) {
  ch <- list()
  
  for(k in 2:K) {
    ch1 <- (clusters[[k]]$betweenss / (k - 1)) / (clusters[[k]]$tot.withinss / (length(clusters[[k]]$cluster- k)))
    ch2 <- cluster.stats(distance, clusters[[k]]$cluster,
                                silhouette = F, G2 = FALSE, G3 = FALSE,
                                wgap=F, sepindex=F, sepprob=0.1,
                                sepwithnoise=F)
    ch_d <- ch2$ch
    
    ch = append(ch , list(k = k, 
                     ch = ch1,
                     ch_d = ch_d))
  }
  
  chPlot <- unlist(ch) %>%
  matrix(3, (K-1)) %>%
  t() %>%
  as.data.frame %>%
  mutate(dif = abs(V2 - V3))
  
  names(chPlot) <- c("k", "ch1", "ch2", "dif")

  
  return(chPlot)
}

```

### Number of Clusters
Selecting the number of clusters, $k$, is one of the most difficult problems in cluster analysis (Steinley, 2006). To date, the most extensive investigation of methods for determining $k$ was conducted by Milligan and Cooper (1985) who analyzed 30 methods. However, aside from the limited generalizability of this study, many methods are also innapropriate in the context of non-hierarchical and thus do not support k-means clustering. Hennig and Liao (2013) argue that the method of selecting $k$ should depend on the context of the clustering and frame the issue as one of obtaining an appropriate subject-matter-dependent definition of rather than a statistical estimation.

* Everitt (2011), p126
  * clusterSim
  * Continuous data?
    * Calinski and Harabasz (1974)
    * Duda and Hart (1973)
  * Steinley, D. (2006a) K-means clustering: a half-century synthesis. British Journal of Mathematical
& Statistical Psychology, 59, 1â€“34.  
  
* Milligan and Cooper (1984)
  + list 30

### Subs-Full


```{r SUBS_FULL, eval = F}
# K <- 20
# 
# distance <- df[, subs_f_vars[-2]] %>%
#   mutate(MEDINC = log(MEDINC),
#          n = log(n)) %>%
#   # sample_n(100) %>%
#   daisy(metric = "gower")
# 
# 
# clust_time <- system.time(
#   clusters <- run_clusters(K, distance)
# )
# 
# ch_time <- system.time(
#   chPlot <- get_CH(K, distance, clusters)
# )
# 
# # undebug(get_CH)
# 
# save(clusters, chPlot, clust_time, ch_time, K, distance, file =  "Paper Data/clusters-full-logs.rdata")
```

```{r SUBS_FULL_CH}
# load("clusters-full.rdata")
load("Paper Data/clusters-full-logs.rdata")


chPlot %>%
  gather(key = method, value = value, ch1:dif) %>%
  ggplot(aes(x = k, y = value, color = method)) +
  geom_point() +
  geom_line() +
  theme_apa() +
  scale_x_discrete(limits = c(1:K))



```

```{r SUBS_FULL_SSB/SST}
cls <- bind_cols(lapply(clusters, function(x) data.frame(x$cluster)))
vrat <- unlist(lapply(clusters, function(x) x$betweenss / x$totss))

data.frame(k = 1:K, var = vrat, min80 = sum(vrat < .8) + .5) %>%
  ggplot(aes(x = k, y = var)) +
  geom_point() +
  ggtitle("Between cluster variance by number of strata") +
  labs(y = "Between Cluster Variance",
       x = "Number of Strata (k)") +
  geom_line() +
  geom_vline(aes(xintercept = min80), linetype = "dashed") +
  theme_minimal() +
  scale_x_discrete(limits = c(1:K)) +
  scale_y_continuous(breaks = seq(0, 1, .1))

df$cluster_full_6 <- cls[,6]
df$cluster_full_10 <- cls[,10]
# dist_full <- distance

```

### Subs-OV


```{r SUBS_OV, eval = F}

# K <- 20
# 
# distance <- df[, subs_f_vars[c(-2, -9)]] %>%
#   mutate(n = log(n)) %>% 
#   daisy(metric = "gower")
# 
# 
# clust_time <- system.time(
#   clusters <- run_clusters(K, distance)
# )
# 
# ch_time <- system.time(
#   chPlot <- get_CH(K, distance)
# )
# 
# 
# save(clusters, chPlot, clust_time, ch_time, K, distance, file =  "Paper Data/clusters-OV-logs.rdata")

```

```{r SUBS_OV_CH}
# load("clusters-OV.rdata")
load("Paper Data/clusters-OV-logs.rdata")


chPlot %>%
  gather(key = method, value = value, ch1:dif) %>%
  ggplot(aes(x = k, y = value, color = method)) +
  geom_point() +
  geom_line()



```

```{r SUBS_OV_SSB/SST}
cls <- bind_cols(lapply(clusters, function(x) data.frame(x$cluster)))
vrat <- unlist(lapply(clusters, function(x) x$betweenss / x$totss))

data.frame(k = 1:K, var = vrat, min80 = sum(vrat < .8) + .5) %>%
  ggplot(aes(x = k, y = var)) +
  geom_point() +
  ggtitle("Between cluster variance by number of strata") +
  labs(y = "Between Cluster Variance",
       x = "Number of Strata (k)") +
  geom_line() +
  geom_vline(aes(xintercept = min80), linetype = "dashed") +
  theme_minimal() +
  scale_x_discrete(limits = c(1:K)) +
  scale_y_continuous(breaks = seq(0, 1, .1))

df$cluster_OV_6 <- cls[,6]
df$cluster_OV_10 <- cls[,10]
```

```{r}
orig <- df
df <- orig
```


```{r class_full}
df %>%
  select(cluster_full_6:cluster_OV_10) %>%
  rbind(rbind(mutate(., cluster_full_6 = 20),
              mutate(., cluster_full_10 = 20))) %>%
  group_by(cluster_full_6, cluster_full_10) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  # mutate(p = n/sum(n)) %>%
  spread(key = cluster_full_6, value = n)

```

```{r class_OV}
# df %>%
#   select(cluster_full_6:cluster_OV_10) %>%
#   rbind(rbind(mutate(., cluster_OV_6 = 20), 
#               mutate(., cluster_OV_10 = 20))) %>%
#   group_by(cluster_OV_6, cluster_OV_10) %>%
#   summarise(n = n()) %>%
#   ungroup() %>%
#   # mutate(p = n/sum(n)) %>%
#   spread(key = cluster_OV_6, value = n)

```

### 6 Clusters

```{r class_k6}
df %>%
  select(cluster_full_6:cluster_OV_10) %>%
  rbind(rbind(mutate(., cluster_full_6 = 20),
  mutate(., cluster_OV_6 = 20))) %>%
  group_by(cluster_full_6, cluster_OV_6) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  # mutate(p = n/sum(n)) %>%
  spread(key = cluster_full_6, value = n)

similar <- df %>%
  select(cluster_full_6:cluster_OV_10) %>%
  group_by(cluster_full_6, cluster_OV_6) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

similar

similar <- rbind(similar[c(1:5),1:2], c(cluster_full_6 = 4, cluster_OV_6 = 4)) %>% arrange(cluster_full_6)

df$cluster_OV_6 <- factor(df$cluster_OV_6)
df$cluster_full_6 <- factor(df$cluster_full_6)

levels(df$cluster_full_6) <- similar$cluster_OV_6
```



```{r}
df %>%
  select(urbanicity, pELA, pMath, cluster_full_6, cluster_OV_6) %>%
  gather(key = full_ov, value = cluster, cluster_full_6:cluster_OV_6) %>%
  ggplot(aes(x = pELA, y = pMath, color = as.factor(cluster))) +
  geom_point() +
  facet_grid(urbanicity ~ full_ov) +
  scale_color_brewer(type = "div")

```

```{r}
df %>%
  select(urbanicity, pELL, pED, cluster_full_6, cluster_OV_6) %>%
  gather(key = full_ov, value = cluster, cluster_full_6:cluster_OV_6) %>%
  ggplot(aes(x = pELL, y = pED, color = cluster)) +
  geom_point() +
  facet_grid(urbanicity ~ full_ov) +
  scale_color_brewer(type = "div")

```

### 10 Clusters

```{r class_k10}
df %>%
  select(cluster_full_10:cluster_OV_10) %>%
  rbind(rbind(mutate(., cluster_full_10 = 20), 
              mutate(., cluster_OV_10 = 20))) %>%
  group_by(cluster_full_10, cluster_OV_10) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  # mutate(p = n/sum(n)) %>%
  spread(key = cluster_full_10, value = n)

similar <- df %>%
  select(cluster_full_6:cluster_OV_10) %>%
  group_by(cluster_full_10, cluster_OV_10) %>%
  summarise(n = n()) %>%
  arrange(desc(n))

similar <- similar[c(1:10),1:2]

df$cluster_OV_10 <- factor(df$cluster_OV_10)
df$cluster_full_10 <- factor(df$cluster_full_10)

levels(df$cluster_full_10) <- similar$cluster_OV_10

```

```{r}
df %>%
  select(urbanicity, pELA, pMath, cluster_full_10, cluster_OV_10) %>%
  gather(key = full_ov, value = cluster, cluster_full_10:cluster_OV_10) %>%
  ggplot(aes(x = pELA, y = pMath, color = as.factor(cluster))) +
  geom_point() +
  facet_grid(urbanicity ~ full_ov) +
  scale_color_brewer(type = "div")

```

```{r}
df %>%
  select(urbanicity, pELL, pED, cluster_full_10, cluster_OV_10) %>%
  gather(key = full_ov, value = cluster, cluster_full_10:cluster_OV_10) %>%
  ggplot(aes(x = pELL, y = pED, color = cluster)) +
  geom_point() +
  facet_grid(urbanicity ~ full_ov) +
  scale_color_brewer(type = "div")

```

\newpage

# References
```{r create_r-references}
# r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
