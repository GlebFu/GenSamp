---
title             : "Analysis Write-up"
shorttitle        : "Analysis Write-up"

author:
  - name          : "Author"
    affiliation   : "1"
  #   corresponding : yes    # Define only one corresponding author
  #   address       : "Postal address"
  #   email         : "my@email.com"
  # - name          : "Ernst-August Doelle"
  #   affiliation   : "1,2"

affiliation:
  - id            : "1"
    institution   : "School/Bakery"
#   - id            : "2"
#     institution   : "Konstanz Business School"

# author_note: |
#   Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

#   Enter author note here.

# abstract: |
#   Enter abstract here. Each new line herein must be indented, like this line.
  
# keywords          : "keywords"
# wordcount         : "X"

# bibliography      : ["r-references.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = F)
```

```{r load.packages, include = FALSE}
library(papaja)
library(tidyverse)

rm(list = ls())
```

```{r analysis_preferences}
# Seed for random number generation
# set.seed(42)
```

```{r data}
load("data/base data.rdata")

vars <- c("LSTATE", "LEANM", "SCHNAM", "DID", "SID", "DSID", "n", "pTotfrl",
          "Urban", "Suburban", "ToRu",
          "pELL", "pED", "pELA", "pMath", "pMin", "MEDINC", "MEDINC_SD")

# Covarites 
subs_f_vars <- c("n", "pTotfrl", "urbanicity",
                "pELL", "pED", "pELA", "pMath", "pMin", "MEDINC")

subs_ov_vars <- c("n", "pTotfrl", "urbanicity",
                "pELL", "pED", "pELA", "pMath", "pMin")

# School Level Data
df.sch <- df[,vars]


# District Level Data About students
df.dist <- list(.vars = lst("n", vars[8:17], "m"),
     .funs = lst(mean, funs(weighted.mean(., w = n)), mean)) %>%
  pmap(~ df.sch %>%
         mutate(m = n()) %>%
         group_by(DID) %>%
         summarise_at(.x, .y)) %>%
  reduce(inner_join, by = "DID")


```

```{r descriptives}
tbl_desc <- df.dist %>%
  gather(key = Variable, value = Value, n:MEDINC, m) %>%
  group_by(Variable) %>%
  summarise(M = mean(Value), SD = sd(Value))

tbl_desc <- df.sch %>%
  gather(key = Variable, value = Value, n:MEDINC) %>%
  group_by(Variable) %>%
  summarise(M = mean(Value), SD = sd(Value))  %>%
  right_join(tbl_desc, by = "Variable")



tbl_desc$Variable <- c("Number of Schools", "Median Income", "School Size", "Economically Disadvantaged", "ELA Proficiency", "English Language Learners", "Math Proficiency", "Minority Status", "Total/Free/Reduced Lunch", "Suburban", "Town or Rural", "Urban")

tbl_desc <- tbl_desc[c(1,3,2,5,7,4,6,8,9,12,10,11),]


```

```{r tbl-desc, results = "asis"}

apa_table(tbl_desc,
          caption = "Descriptives of variables",
          note = "District variables are derived as aggregate means of school variables",
          stub_indents = list(`Average Proportions` = c(4:9), Indicators = c(10:12)),
          col_spanners = list(`School` = c(2,3), `District Weighted` = c(4,5)),
          align = "lcccc",
          col.names = c("Variables","Mean", "SD", "Mean", "SD"))

```


# Cluster Analysis


## Population Frame
The population frame is composed of data from three sources: (1) the Common Core of Data (CCD), (2) publicly available accountability data, and (3) the U.S. Census. The CCD is a comprehensive database housing annually collected national statistics of all public schools and districts. Accountability data was used to calculate the proportion of students within each school performing at or above proficiency in Math and ELA. Finally, local median income was obtained from the U.S. Census and was matched to each school by zip code. 
### Covariates
Selection of covariates was driven by prior research on district and school participation behavior in RCTs (Stuart et al., 2017; Tipton et al., 2016a; Fellers, 2017). Districts and schools with higher proportions of students who are English language learners (ELL), economically disadvantaged (ED), non-White, and living in urban settings are more likely to participate, as are larger districts and schools. It is important to note, however, that some of these characteristics might also make it more likely that researchers would recruit these districts and schools in the first place. Anecdotal evidence from several research teams also suggests that schools are less willing to participate in experimental interventions for subjects in which their students are already excelling, therefore math and ELA achievement covariates were also included. Weighted means of school level variables were calculated using school size (number of students) to generate district level covariates. In this sense, district covariates describe the population of students rather than the population of schools. Descriptives of these covariates are reported in table \@ref(tab:tbl-desc)

### Omitted Variable
Neighborhood median income was selected to serve as the omitted variable for several reasons. First, it is related to several of the other variables and therefore likely predicts school or district participation. Second, because it comes from a non-educational data source (the census) and requires additional work to include in the population frame it is likely to be omitted in practice.

```{r dists, results = "asis", fig.cap="Comparison of covariate distributions and their log transformations."}
to_z <- function(x) (x - mean(x))/sd(x)

dist_plots <- df[, c("DSID",subs_f_vars[-2])] %>%
  mutate(n_log = log(n),
         MEDINC_log = log(MEDINC/1000),
         MEDINC = MEDINC / 1000)

dist_plots %>%
  gather(key = var, value = value, n, n_log, MEDINC, MEDINC_log) %>%
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ var, scales = "free") +
  theme_apa()

# dist_plots %>%
#   gather(key = var, value = value, n:MEDINC_log, -urbanicity) %>%
#   ggplot(aes(x = value, color = urbanicity)) +
#   geom_density() +
#   facet_wrap(~ var, scales = c("free"))
# 
# dist_plots %>%
#   ggplot(aes(x = pELA, y = pMath, color = n_log)) +
#   geom_point(alpha = .5) +
#   facet_wrap(~urbanicity)
# 
# dist_plots %>%
#   ggplot(aes(x = pELL, y = pMin, color = n_log)) +
#   geom_point(alpha = .5) +
#   facet_wrap(~urbanicity)
# 
# dist_plots %>%
#   ggplot(aes(x = MEDINC_log, y = pED, color = n_log)) +
#   geom_point(alpha = .5) +
#   facet_wrap(~urbanicity)
```

```{r dists2, fig.cap="Distributions of the remaining continuous covariates."}
dist_plots %>%
  gather(key = var, value = value, pED:pMin) %>%
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ var, scales = "free") +
  theme_apa()

```

### Variable Transformations
Log-transformation was used on school size (number of students) and median income. This is done to allow proportional comparisons at the extremes of the distributions (Hennig and Liao - 2013). For instance, the difference between two schools with 4000 and 3000 students should be weighed as much as the difference between two schools with 400 and 300 students when generating clusters. Figure \@ref(fig:dists) displays a comparison of the distribution of these variables and their logs. Figure \@ref(fig:dists2) displays the distribution of the remaining continuous variables.


## SUBS

Stratification using balanced sampling (SUBS) was performed prior to simulation because the group of schools in each strata would be static across conditions except where the balancing model is manipulated by omitting median income. Per Tipton's (2013) original recommendation, we use k-means clustering to partition the population into strata. This requires selecting a distance metric, choosing the number of strata, and performing the actual analysis.

### Distance Metric

The set of covariates in both the full model (SUBS-F) and the omitted variable model (SUBS-OV) include continuous covariates as well as binary indicators for urbanicity (urban, suburban, and town/rural). Within this context it is generally recommended to use Gower's (1971) general dissimilarity distance (Everitt, 2011) which Tipton (2013) echos. This method relies on different calculations of distance depending on the type of covariates. Let $d_{ii'h}$ be the distance between observed values of covariate $X_{h}$ for unit $i$ and unit $i'$ where $i \ne i'$. For categorical or dummy coded variables, $d_{ii'h} = 1$ if $X_{ih} = X_{i'h}$ and $d_{ii'h} = 0$ otherwise. For continuous covariates, we use the following formula:
\begin{align}
  d_{ii'h} = 1 - \frac{|X_{ih} - X_{i'h}|}{R_h}
\end{align}
where |.| indicates absolute value, $X_{ih}$ and $X_{i'h}$ are values of the $h^{th}$ covariate for units $i$ and $i'$, and $R_h$ is the range of observations for covariate $X_h$. This method restricts the range of $d_{ii'h}$ to [0,1]. Finally, we calculate the general similarity between each unit pair by taking the weighted average of the distances between all covariates. Let $d^{g}_{ii'}$ be the general similarity between unit $i$ and unit $i'$ where $i \ne i'$.  

\begin{align}
  d^{g}_{ii'} = \frac{\sum^p_{h = 1}w_{ii'h}d_{ii'h}}{\sum^p_{h = 1}w_{ii'h}}
\end{align}
where $w_{ii'h} = 0$ if $X_h$ is missing for either unit and $w_{ii'h} = 1$ otherwise. This produces an $n$ by $n$ distance (or dissimilarity) matrix.

### Number of Clusters
Selecting the number of clusters, $k$, is one of the most difficult problems in cluster analysis (Steinley, 2006). To date, the most extensive investigation of methods for determining $k$ was conducted by Milligan and Cooper (1985) who analyzed 30 methods. However, aside from the limited generalizability of this study, many methods are also inappropriate in the context of non-hierarchical clustering and thus do not support k-means clustering. Tipton (2013) states that both statistical and practical criteria should be used in selecting the number of clusters. Specifically, a large number of clusters would result in more homogeneous strata and, in turn, a more robust sample. However as strata become smaller they also become more difficult to adequately sample from. Hennig and Liao (2013) also argue that the method of selecting $k$ should depend on the context of the clustering and frame the issue as one of obtaining an appropriate subject-matter-dependent definition of rather than a statistical estimation. Ultimately three considerations were used to select the number of clusters: the ratio of variability between clusters to the sum of within and between cluster variability as recommended by Tipton (2013), a generalized form of the Calinski-Harabasz index (Calinski and Harabasz, 1974) proposed by Hennig and Liao (2013), and the practicality of sampling from fewer clusters.

* Everitt (2011), p126
  * clusterSim
  * Continuous data?
    * Calinski and Harabasz (1974)
    * Duda and Hart (1973)
  * Steinley, D. (2006) K-means clustering: a half-century synthesis. British Journal of Mathematical
& Statistical Psychology, 59, 1–34.  
  
* Milligan and Cooper (1984)
  + list 30
* Sugar and James (2003) via Hennig & Liao 2013 p 314
  + Modern look

### Cluster Analysis
Cluster analysis was performed using the _cluster_ package (Maechler et. al. 2017) in R. First, the _daisy_ function is used to compute an $n$ by $n$ pairwise distance matrix across all observations. This function requires two parameters: (1) the data set, and (2) the distance metric. For the full model, the data set included the full list of school level covariates presented in table \@ref(tab:tbl-desc). For the omitted variable model, median income was omitted from the data set. For both models, the metric was set to "gower". Next the _kmeans_ function is used to generate clusters. This method uses an optimization algorithm to classify units into $k$ clusters by minimizing the total within cluster sum of squares. This function also requires two parameters: (1) the distance matrix, and (2) the number of clusters to generate ($k$). For each $k$, it is recommended to run _kmeans_ at least 10 times, and select the clustering that results in the smallest total within-cluster sum of squares. [Get Citation]

Figure \@ref(fig:ch-full) displays the Calinski-Harabasz (CH) index for each $k$ clusters generated for both the SUBS-F and SUBS-OV. The value of $k$ that maximizes the CH index should be selected. However we see several local maxima: $k = [2, 7, 10]$ for SUBS-F, and $k = 2, 6, 12$ for SUBS-OV.

Taking the ratio of between-cluster SS to within-cluster SS and plotting it against number of clusters creates a chart similar to an upside down elbow graph. Figure \@ref(fig:ratio-full) displays this for both SUBS-F and SUBS-OV. Tipton (2013) recommends selecting the number of clusters such that at least 80% of the variability is between clusters, indicated by the figure as a dashed line. Given this criteria it seems that in both models at least 10 clusters should be generated. However we also see that after a sharp initial increase, the slope of the graph begins to level out. This indicates that as we increase the number of clusters, the benefit of doing so decreases, while the difficulty of sampling from each cluster increases. In that case after 6 or 7 clusters the difficulty of sampling may not be worth such small increases in homogeneity within clusters.

Figure \@ref(fig:k-size-full plots the sample size that needs to be selected from each cluster to fulfill the proportional allocation requirement such that the number of units sampled from each cluster is proportional to the size of the cluster in the population. The dashed line indicates the ideal allocation if all clusters were of equal size. We see that for SUBS-F when 8 or less clusters are generated, they are more equally sized, with the exception of 3 and 6 clusters where one is much larger than the others. For SUBS-OV this is less apparent. Instead a sensible cutoff may be determined by looking at the size of the smallest cluster. At $k > 7$ it seems that the smallest clusters would require less than 5 units being sampled, which may be very difficult in a practical setting.

In order to maintain comparability between methods, it was determined that 6 clusters would be generated for both models, though in practice 7 clusters for the full model may be more prudent.

```{r load_clusterData}
load("Paper Data/clusters-full-logs.rdata")
ch_f <- chPlot
clusters_f <- clusters

ch_f$subs <- "F"

load("Paper Data/clusters-OV-logs.rdata")
ch_ov <- chPlot
clusters_ov <- clusters

ch_ov$subs <- "OV"

chPlot <- rbind(ch_f, ch_ov)
```

```{r ch-full, fig.cap="Generalizd Calinski-Harabasz index"}
chPlot %>%
  gather(key = method, value = value, ch2) %>%
  ggplot(aes(x = k, y = value)) +
  geom_point() +
  geom_line() +
  theme_apa() +
  scale_x_discrete(limits = c(1:K)) +
  facet_grid(subs ~ .)


```



```{r ratio-full, fig.cap="Ratio of between cluster sum of squares to total cluster sum of squares"}
cls_ov <- bind_cols(lapply(clusters_ov, function(x) data.frame(x$cluster)))
cls_f <- bind_cols(lapply(clusters_f, function(x) data.frame(x$cluster)))

ratio_data <- rbind(data.frame(k = 1:K, subs = "F", vrat = unlist(lapply(clusters_f, function(x) x$betweenss / x$totss))),
                    data.frame(k = 1:K, subs = "OV", vrat = unlist(lapply(clusters_ov, function(x) x$betweenss / x$totss))))

ratio_data %>%
  group_by(subs) %>%
  mutate(min80 = sum(vrat < .8) + .5) %>%
  ungroup() %>%
  ggplot(aes(x = k, y = vrat)) +
  geom_point() +
  labs(y = "Between Cluster Variance",
       x = "Number of Strata (k)") +
  geom_line() +
  geom_vline(aes(xintercept = min80), linetype = "dashed") +
  theme_apa() +
  scale_x_discrete(limits = c(1:K)) +
  scale_y_continuous(breaks = seq(0, 1, .1)) +
  facet_grid(subs ~ .)
```

```{r k-size-full, fig.cap="Sampling requirements for each cluster"}
names(cls_ov) <- names(cls_f) <- 1:K
cls_ov$subs <- "OV"
cls_f$subs <- "F"


rbind(cls_f, cls_ov) %>%
  gather(key = k, value = cluster, -subs) %>%
  filter(k > 1) %>%
  mutate(k = as.numeric(k)) %>%
  group_by(k, cluster, subs) %>%
  summarise(n = n()) %>%
  group_by(k, subs) %>%
  mutate(sample = (n / sum(n)) * 60) %>%
  ggplot(aes(x = k, y = sample)) +
  geom_point() +
  theme_apa() +
  labs(y = "Allocated Sample Size",
       x = "Number of Strata (k)") +
  scale_x_discrete(limits = c(1:K)) +
  scale_y_continuous(breaks = seq(0, 30, 5)) +
  stat_function(fun = function(x) 60/x, geom = "line", linetype = "dashed") +
  facet_grid(subs ~ .)

```

```{r k-plots}
# multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
#   library(grid)
# 
#   # Make a list from the ... arguments and plotlist
#   plots <- c(list(...), plotlist)
# 
#   numPlots = length(plots)
# 
#   # If layout is NULL, then use 'cols' to determine layout
#   if (is.null(layout)) {
#     # Make the panel
#     # ncol: Number of columns of plots
#     # nrow: Number of rows needed, calculated from # of cols
#     layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
#                     ncol = cols, nrow = ceiling(numPlots/cols))
#   }
# 
#  if (numPlots==1) {
#     print(plots[[1]])
# 
#   } else {
#     # Set up the page
#     grid.newpage()
#     pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# 
#     # Make each plot, in the correct location
#     for (i in 1:numPlots) {
#       # Get the i,j matrix positions of the regions that contain this subplot
#       matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
# 
#       print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
#                                       layout.pos.col = matchidx$col))
#     }
#   }
# }
# 
# multiplot(ch_full, ratio_full, k_size_full, cols = 1)
```

\newpage

# References
```{r create_r-references}
# r_refs(file = "r-references.bib")
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
