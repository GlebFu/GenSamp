@article{tipton2020toward,
  title={Toward a System of Evidence for All: Current Practices and Future Opportunities in 37 Randomized Trials},
  author={Tipton, Elizabeth and Spybrook, Jessaca and Fitzgerald, Kaitlyn G and Wang, Qian and Davidson, Caryn},
  journal={Educational Researcher},
  pages={0013189X20960686},
  year={2020},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@Manual{rcite,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2017},
  url = {https://www.R-project.org/},
}
@article{raudenbush2015learning,
  title={Learning about and from a distribution of program impacts using multisite trials},
  author={Raudenbush, Stephen W and Bloom, Howard S},
  journal={American Journal of Evaluation},
  volume={36},
  number={4},
  pages={475--499},
  year={2015},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@Article{tiptonImprovingGeneralizationsExperiments2013,
  title = {Improving {{Generalizations From Experiments Using Propensity Score Subclassification}}: {{Assumptions}}, {{Properties}}, and {{Contexts}}},
  volume = {38},
  issn = {1076-9986},
  url = {https://www.jstor.org/stable/41999424},
  shorttitle = {Improving {{Generalizations From Experiments Using Propensity Score Subclassification}}},
  abstract = {As a result of the use of random assignment to treatment, randomized experiments typically have high internal validity. However, units are very rarely randomly selected from a well-defined population of interest into an experiment; this results in low external validity. Under nonrandom sampling, this means that the estimate of the sample average treatment effect calculated in the experiment can be a biased estimate of the population average treatment effect. This article explores the use of the propensity score subclassification estimator as a means for improving generalizations from experiments. It first lays out the assumptions necessary for generalizations, then investigates the amount of bias reduction and average variance inflation that is likely when compared to a conventional estimator. It concludes with a discussion of issues that arise when the population of interest is not well represented by the experiment, and an example.},
  number = {3},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  urldate = {2018-12-05},
  date = {2013},
  pages = {239-266},
  author = {Elizabeth Tipton},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\62FUAEDQ\\Tipton - 2013 - Improving Generalizations From Experiments Using P.pdf},
}

@Article{tiptonStratifiedSamplingUsing2013,
  langid = {english},
  title = {Stratified {{Sampling Using Cluster Analysis}}: {{A Sample Selection Strategy}} for {{Improved Generalizations From Experiments}}},
  volume = {37},
  issn = {0193-841X},
  url = {https://doi.org/10.1177/0193841X13516324},
  doi = {10.1177/0193841X13516324},
  shorttitle = {Stratified {{Sampling Using Cluster Analysis}}},
  abstract = {Background:An important question in the design of experiments is how to ensure that the findings from the experiment are generalizable to a larger population. This concern with generalizability is particularly important when treatment effects are heterogeneous and when selecting units into the experiment using random sampling is not possible?two conditions commonly met in large-scale educational experiments.Method:This article introduces a model-based balanced-sampling framework for improving generalizations, with a focus on developing methods that are robust to model misspecification. Additionally, the article provides a new method for sample selection within this framework: First units in an inference population are divided into relatively homogenous strata using cluster analysis, and then the sample is selected using distance rankings.Result:In order to demonstrate and evaluate the method, a reanalysis of a completed experiment is conducted. This example compares samples selected using the new method with the actual sample used in the experiment. Results indicate that even under high nonresponse, balance is better on most covariates and that fewer coverage errors result.Conclusion:The article concludes with a discussion of additional benefits and limitations of the method.},
  number = {2},
  journaltitle = {Evaluation Review},
  shortjournal = {Eval Rev},
  urldate = {2018-12-05},
  date = {2013-04-01},
  pages = {109-139},
  author = {Elizabeth Tipton},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\UBJEYKFR\\Tipton - 2013 - Stratified Sampling Using Cluster Analysis A Samp.pdf},
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2018},
  url = {https://www.R-project.org/},
}
@Manual{R-bindrcpp,
  title = {bindrcpp: An 'Rcpp' Interface to Active Bindings},
  author = {Kirill Müller},
  year = {2017},
  note = {R package version 0.2},
  url = {https://CRAN.R-project.org/package=bindrcpp},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain Francois and Lionel Henry and Kirill Müller},
  year = {2017},
  note = {R package version 0.7.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2018},
  note = {R package version 0.3.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Manual{fpc,
	title = {fpc: Flexible Procedures for Clustering},
	author = {Christian Hennig},
	year = {2020},
	note = {R package version 2.2-7},
	url = {https://CRAN.R-project.org/package=fpc},
}
@Article{R-Formula,
  title = {Extended Model Formulas in {R}: Multiple Parts and Multiple Responses},
  author = {Achim Zeileis and Yves Croissant},
  journal = {Journal of Statistical Software},
  year = {2010},
  volume = {34},
  number = {1},
  pages = {1--13},
  doi = {10.18637/jss.v034.i01},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2009},
  isbn = {978-0-387-98140-6},
  url = {http://ggplot2.org},
}
@Manual{R-Hmisc,
  title = {Hmisc: Harrell Miscellaneous},
  author = {Frank E {Harrell Jr} and with contributions from Charles Dupont and many others.},
  year = {2018},
  note = {R package version 4.1-1},
  url = {https://CRAN.R-project.org/package=Hmisc},
}
@Manual{R-htmlTable,
  title = {htmlTable: Advanced Tables for Markdown/HTML},
  author = {Max Gordon and Stephen Gragg and Peter Konings},
  year = {2018},
  note = {R package version 1.11.2},
  url = {https://CRAN.R-project.org/package=htmlTable},
}
@Book{R-lattice,
  title = {Lattice: Multivariate Data Visualization with R},
  author = {Deepayan Sarkar},
  publisher = {Springer},
  address = {New York},
  year = {2008},
  note = {ISBN 978-0-387-75968-5},
  url = {http://lmdvr.r-forge.r-project.org},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2018},
  note = {R package version 0.1.0.9842},
  url = {https://github.com/crsh/papaja},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Lionel Henry and Hadley Wickham},
  year = {2017},
  note = {R package version 0.2.4},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Romain Francois},
  year = {2017},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-stargazer,
  title = {stargazer: Well-Formatted Regression and Summary Statistics Tables},
  author = {Marek Hlavac},
  year = {2018},
  note = {R package version 5.2.1},
  organization = {Central European Labour Studies Institute (CELSI)},
  address = {Bratislava, Slovakia},
  url = {https://CRAN.R-project.org/package=stargazer},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2018},
  note = {R package version 1.3.0},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Book{R-survival-book,
  title = {Modeling Survival Data: Extending the {C}ox Model},
  author = {{Terry M. Therneau} and {Patricia M. Grambsch}},
  year = {2000},
  publisher = {Springer},
  address = {New York},
  isbn = {0-387-98784-3},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2018},
  note = {R package version 1.4.2},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Easily Tidy Data with 'spread()' and 'gather()' Functions},
  author = {Hadley Wickham and Lionel Henry},
  year = {2018},
  note = {R package version 0.8.0},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Manual{R-tidyverse,
  title = {tidyverse: Easily Install and Load the 'Tidyverse'},
  author = {Hadley Wickham},
  year = {2017},
  note = {R package version 1.2.1},
  url = {https://CRAN.R-project.org/package=tidyverse},
}
@Manual{R-xtable,
  title = {xtable: Export Tables to LaTeX or HTML},
  author = {David B. Dahl},
  year = {2016},
  note = {R package version 1.8-2},
  url = {https://CRAN.R-project.org/package=xtable},
}
@Article{stuartUsePropensityScores2011,
  langid = {english},
  title = {The Use of Propensity Scores to Assess the Generalizability of Results from Randomized Trials: {{Use}} of {{Propensity Scores}} to {{Assess Generalizability}}},
  volume = {174},
  issn = {09641998},
  url = {http://doi.wiley.com/10.1111/j.1467-985X.2010.00673.x},
  doi = {10.1111/j.1467-985X.2010.00673.x},
  shorttitle = {The Use of Propensity Scores to Assess the Generalizability of Results from Randomized Trials},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  urldate = {2018-12-05},
  date = {2011-04},
  pages = {369-386},
  author = {Elizabeth A. Stuart and Stephen R. Cole and Catherine P. Bradshaw and Philip J. Leaf},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\Z2VSWX4P\\Stuart et al. - 2011 - The use of propensity scores to assess the general.pdf},
}
@Book{shadishExperimentalQuasiexperimentalDesigns2002,
  address = {Boston, MA, US},
  series = {Experimental and quasi-experimental designs for generalized causal inference},
  title = {Experimental and Quasi-Experimental Designs for Generalized Causal Inference},
  isbn = {978-0-395-61556-0},
  abstract = {This is a book for those who have already decided that identifying a dependable relationship between a cause and its effects is a high priority and who wish to consider experimental methods for doing so. Such causal relationships are of great importance in human affairs. The rewards associated with being correct in identifying causal relationships can be high, an the costs of misidentification can be tremendous. This book has two major purposes: to describe ways in which testing causal propositions can be improved in specific research projects, and to describe ways to improve generalizations about causal propositions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  publisher = {{Houghton, Mifflin and Company}},
  author = {William R. Shadish and Thomas D. Cook and Donald T. Campbell},
  year = {2002},
  keywords = {Causal Analysis,Experimental Design,Experimental Methods,Quasi Experimental Methods},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\MDZLE8PQ\\2002-17373-000.html},
}
@Article{olsenExternalValidityPolicy2013,
  title = {External {{Validity}} in {{Policy Evaluations That Choose Sites Purposively}}},
  volume = {32},
  copyright = {\textcopyright{} 2012 by the Association for Public Policy Analysis and Management},
  issn = {1520-6688},
  doi = {10.1002/pam.21660},
  abstract = {Evaluations of the impact of social programs are often carried out in multiple sites, such as school districts, housing authorities, local TANF offices, or One-Stop Career Centers. Most evaluations select sites purposively following a process that is nonrandom. Unfortunately, purposive site selection can produce a sample of sites that is not representative of the population of interest for the program. In this paper, we propose a conceptual model of purposive site selection. We begin with the proposition that a purposive sample of sites can usefully be conceptualized as a random sample of sites from some well-defined population, for which the sampling probabilities are unknown and vary across sites. This proposition allows us to derive a formal, yet intuitive, mathematical expression for the bias in the pooled impact estimate when sites are selected purposively. This formula helps us to better understand the consequences of selecting sites purposively, and the factors that contribute to the bias. Additional research is needed to obtain evidence on how large the bias tends to be in actual studies that select sites purposively, and to develop methods to increase the external validity of these studies. \textcopyright{} 2012 by the Association for Public Policy Analysis and Management.},
  language = {en},
  number = {1},
  journal = {Journal of Policy Analysis and Management},
  author = {Robert B. Olsen and Larry L. Orr and Stephen H. Bell and Elizabeth A. Stuart},
  year = {2013},
  pages = {107-121},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\A4IXDTZY\\Olsen et al. - 2013 - External Validity in Policy Evaluations That Choos.pdf;C:\\Users\\Gleb\\Zotero\\storage\\TJFGEVLH\\pam.html},
}
@Article{tiptonHowGeneralizableYour2014,
  title = {How {{Generalizable Is Your Experiment}}? {{An Index}} for {{Comparing Experimental Samples}} and {{Populations}}},
  volume = {39},
  issn = {1076-9986},
  shorttitle = {How {{Generalizable Is Your Experiment}}?},
  abstract = {Although a large-scale experiment can provide an estimate of the average causal impact for a program, the sample of sites included in the experiment is often not drawn randomly from the inference population of interest. In this article, we provide a generalizability index that can be used to assess the degree of similarity between the sample of units in an experiment and one or more inference populations on a set of selected covariates. The index takes values between 0 and 1 and indicates both when a sample is like a miniature of the population and how well reweighting methods may perform when differences exist. Results of simulation studies are provided that develop rules of thumb for interpretation as well as an example.},
  number = {6},
  journal = {Journal of Educational and Behavioral Statistics},
  author = {Elizabeth Tipton},
  year = {2014},
  pages = {478-501},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\C929YILU\\Tipton - 2014 - How Generalizable Is Your Experiment An Index for.pdf},
}
@Thesis{fellersDevelopingApproachDetermine2017,
  langid = {english},
  location = {{United States -- New York}},
  title = {Developing an Approach to Determine Generalizability: {{A}} Review of Efficacy and Effectiveness Trials Funded by the {{Institute}} of {{Education Sciences}}},
  url = {https://search.proquest.com/docview/1865595768/abstract/40FD82F4A0C24535PQ/1},
  shorttitle = {Developing an Approach to Determine Generalizability},
  abstract = {Since its establishment the Institute of Education Sciences has been creating opportunities and driving standards to generate research in education that is high quality rigorous, and relevant. This dissertation is an analysis of current practices in Goal III and Goal IV studies, in order to (1) better understand of the types of schools that agree to take part in these studies, and (2) an assess how representative these schools are in comparison to important policy relevant populations. This dissertation focuses on a subset of studies that were funded from 2005-2014 by the Department of Education, IES, under the NCER grants-funding arm. Studies included were those whose interventions were aimed at elementary students across core curriculum and ELL program areas. Study schools were compared to two main populations, the U.S population of elementary schools and Title I elementary schools, as well as these populations on a state level. The B-index, proposed by Tipton (2014) was the main value of comparison used to assess the compositional similarity, or generalizability, of study schools to these identified inference populations. The findings show that across all studies included in this analysis, participating schools were representative of the U.S. population of schools, B-index = 0.9. Comparisons were also made between this collection of schools and the respective populations at the state level. Results showed that these schools were not representative of any individual states (no B-index values were greater than 0.90). Across all included studies, schools that agreed to participate were more often located in urban areas, had higher rates of FRL students, had more minority students enrolled, and had more total students, in both district and school, than those schools in the population of U.S. schools. It is clear that the movement of education research is to be relevant to a larger audience. Through this study it is clear that, across studies, we are achieving some representation in IES funded studies. However, the finer comparisons, study samples to individual state and individual studies to these populations, show limited similarity between study schools and populations of interest to policy makers using these study findings to make decisions about their schools.},
  pagetotal = {130},
  institution = {{Columbia University}},
  type = {Ph.D.},
  urldate = {2018-12-04},
  date = {2017},
  keywords = {Education,External validity,Generalizability,Pure sciences,Recruitment},
  author = {Lauren Fellers},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\VH9IMSKE\\Fellers - 2017 - Developing an approach to determine generalizabili.pdf},
}

@Article{tiptonSiteSelectionExperiments2016,
  title = {Site {{Selection}} in {{Experiments}}: {{An Assessment}} of {{Site Recruitment}} and {{Generalizability}} in {{Two Scale}}-up {{Studies}}},
  volume = {9},
  issn = {1934-5747},
  url = {https://doi.org/10.1080/19345747.2015.1105895},
  doi = {10.1080/19345747.2015.1105895},
  shorttitle = {Site {{Selection}} in {{Experiments}}},
  abstract = {Recently, statisticians have begun developing methods to improve the generalizability of results from large-scale experiments in education. This work has included the development of methods for improved site selection when random sampling is infeasible, including the use of stratification and targeted recruitment strategies. This article provides the next step in this literature—a template for assessing generalizability after a study is completed. In this template, first records from the recruitment process are analyzed, comparing differences between those who agreed to be in the study and those who did not. Second, the final sample is compared to the original inference population and different possible subsets, with the goal of determining where the results best generalize (and where they do not). Throughout, these methods are situated in the post hoc analysis of results from two scale-up studies. The article ends with a discussion of the use of these methods more generally when reporting results from randomized trials.},
  issue = {sup1},
  journaltitle = {Journal of Research on Educational Effectiveness},
  urldate = {2018-12-05},
  date = {2016-10-03},
  pages = {209-228},
  keywords = {external validity,generalization,recruitment},
  author = {Elizabeth Tipton and Lauren Fellers and Sarah Caverly and Michael Vaden-Kiernan and Geoffrey Borman and Kate Sullivan and Veronica Ruiz {de Castilla}},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\YCNS43HB\\Tipton et al. - 2016 - Site Selection in Experiments An Assessment of Si.pdf;C:\\Users\\gf4929\\Zotero\\storage\\9B5MP5FJ\\19345747.2015.html},
}

@Article{stuartCharacteristicsSchoolDistricts2017,
  title = {Characteristics of {{School Districts That Participate}} in {{Rigorous National Educational Evaluations}}},
  volume = {10},
  issn = {1934-5747},
  url = {https://doi.org/10.1080/19345747.2016.1205160},
  doi = {10.1080/19345747.2016.1205160},
  abstract = {Given increasing interest in evidence-based policy, there is growing attention to how well the results from rigorous program evaluations may inform policy decisions. However, little attention has been paid to documenting the characteristics of schools or districts that participate in rigorous educational evaluations, and how they compare to potential target populations for the interventions that were evaluated. Utilizing a list of the actual districts that participated in 11 large-scale rigorous educational evaluations, we compare those districts to several different target populations of districts that could potentially be affected by policy decisions regarding the interventions under study. We find that school districts that participated in the 11 rigorous educational evaluations differ from the interventions' target populations in several ways, including size, student performance on state assessments, and location (urban/rural). These findings raise questions about whether, as currently implemented, the results from rigorous impact studies in education are likely to generalize to the larger set of school districts—and thus schools and students—of potential interest to policymakers, and how we can improve our study designs to retain strong internal validity while also enhancing external validity.},
  number = {1},
  journaltitle = {Journal of Research on Educational Effectiveness},
  urldate = {2018-12-19},
  date = {2017-01-02},
  pages = {168-206},
  keywords = {Corrigendum,external validity,generalizability,randomized experiment},
  author = {Elizabeth A. Stuart and Stephen H. Bell and Cyrus Ebnesajjad and Robert B. Olsen and Larry L. Orr},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\7KRDW2UM\\Stuart et al. - 2017 - Characteristics of School Districts That Participa.pdf;C:\\Users\\gf4929\\Zotero\\storage\\EVBPF4PN\\19345747.2016.html},
}
@Article{gowerGeneralCoefficientSimilarity1971,
  title = {A {{General Coefficient}} of {{Similarity}} and {{Some}} of {{Its Properties}}},
  volume = {27},
  issn = {0006-341X},
  doi = {10.2307/2528823},
  abstract = {[A general coefficient measuring the similarity between two sampling units is defined. The matrix of similarities between all pairs of sample units is shown to be positive semidefinite (except possibly when there are missing values). This is important for the multidimensional Euclidean representation of the sample and also establishes some inequalities amongst the similarities relating three individuals. The definition is extended to cope with a hierarchy of characters.]},
  number = {4},
  journal = {Biometrics},
  author = {J. C. Gower},
  year = {1971},
  pages = {857-871},
}
@Article{roschelleIntegrationTechnologyCurriculum2010,
  title = {Integration of {{Technology}}, {{Curriculum}}, and {{Professional Development}} for {{Advancing Middle School Mathematics}}: {{Three Large}}-{{Scale Studies}}},
  volume = {47},
  issn = {0002-8312},
  shorttitle = {Integration of {{Technology}}, {{Curriculum}}, and {{Professional Development}} for {{Advancing Middle School Mathematics}}},
  abstract = {[The authors present three studies (two randomized controlled experiments and one embedded quasi-eocperiment) designed to evaluate the impact of replacement units targeting student learning of advanced middle school mathematics. The studies evaluated the SimCalc approach, which integrates an interactive representational technology, paper curriculum, and teacher professional development. Each study addressed both replicability of findings and robustness across Texas settings, with varied teacher characteristics (backgrounds, knowledge, attitudes) and student characteristics (demographics, levels of prior mathematics knowledge). Analyses revealed statistically significant main effects, with student-level effect sizes of .63, .50, and .56. These consistent gains support the conclusion that SimCalc is effective in enabling a wide variety of teachers in a diversity of settings to extend student learning to more advanced mathematics.]},
  number = {4},
  journal = {American Educational Research Journal},
  author = {Jeremy Roschelle and Nicole Shechtman and Deborah Tatar and Stephen Hegedus and Bill Hopkins and Susan Empson and Jennifer Knudsen and Lawrence P. Gallagher},
  year = {2010},
  pages = {833-878},
}
@Article{tiptonImplicationsSmallSamples2017,
  title = {Implications of {{Small Samples}} for {{Generalization}}: {{Adjustments}} and {{Rules}} of {{Thumb}}},
  volume = {41},
  issn = {0193-841X},
  shorttitle = {Implications of {{Small Samples}} for {{Generalization}}},
  doi = {10.1177/0193841X16655665},
  abstract = {Background:Policy makers and researchers are frequently interested in understanding how effective a particular intervention may be for a specific population. One approach is to assess the degree of similarity between the sample in an experiment and the population. Another approach is to combine information from the experiment and the population to estimate the population average treatment effect (PATE).Method:Several methods for assessing the similarity between a sample and population currently exist as well as methods estimating the PATE. In this article, we investigate properties of six of these methods and statistics in the small sample sizes common in education research (i.e., 10?70 sites), evaluating the utility of rules of thumb developed from observational studies in the generalization case.Result:In small random samples, large differences between the sample and population can arise simply by chance and many of the statistics commonly used in generalization are a function of both sample size and the number of covariates being compared. The rules of thumb developed in observational studies (which are commonly applied in generalization) are much too conservative given the small sample sizes found in generalization.Conclusion:This article implies that sharp inferences to large populations from small experiments are difficult even with probability sampling. Features of random samples should be kept in mind when evaluating the extent to which results from experiments conducted on nonrandom samples might generalize.},
  language = {en},
  number = {5},
  journal = {Evaluation Review},
  author = {Elizabeth Tipton and Kelly Hallberg and Larry V. Hedges and Wendy Chan},
  month = {oct},
  year = {2017},
  pages = {472-505},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\ZULTL9AB\\Tipton et al. - 2017 - Implications of Small Samples for Generalization .pdf},
}
@Article{hennigHowFindAppropriate2013,
  title = {How to Find an Appropriate Clustering for Mixed-Type Variables with Application to Socio-Economic Stratification: {{How}} to {{Find}} an {{Appropriate Clustering}}},
  volume = {62},
  issn = {00359254},
  shorttitle = {How to Find an Appropriate Clustering for Mixed-Type Variables with Application to Socio-Economic Stratification},
  doi = {10.1111/j.1467-9876.2012.01066.x},
  language = {en},
  number = {3},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  author = {Christian Hennig and Tim F. Liao},
  month = {may},
  year = {2013},
  pages = {309-369},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\VRJ6M95F\\Hennig and Liao - 2013 - How to find an appropriate clustering for mixed-ty.pdf},
}
@Manual{R-kableExtra,
  title = {kableExtra: Construct Complex Table with 'kable' and Pipe Syntax},
  author = {Hao Zhu},
  year = {2018},
  note = {R package version 0.9.0},
  url = {https://CRAN.R-project.org/package=kableExtra},
}
@Article{raudenbushStatisticalPowerOptimal2000,
  title = {Statistical Power and Optimal Design for Multisite Randomized Trials.},
  volume = {5},
  issn = {1082-989X},
  doi = {10.1037//1082-989X.5.2.199},
  language = {en},
  number = {2},
  journal = {Psychological Methods},
  author = {Stephen W. Raudenbush and Xiaofeng Liu},
  year = {2000},
  pages = {199-213},
}
@Book{grovesSurveyMethodology2004,
  address = {Hoboken, N.J},
  series = {Wiley Series in Survey Methodology},
  title = {Survey Methodology},
  isbn = {978-0-471-48348-9},
  lccn = {HA31.2 .S873 2004},
  publisher = {{Wiley-Interscience}},
  editor = {Robert M. Groves},
  year = {2004},
  keywords = {Methodology,Research Statistical methods,Social sciences,Social surveys,Surveys},
}
@Article{omuircheartaighGeneralizingUnrepresentativeExperiments2014,
  title = {Generalizing from Unrepresentative Experiments: A Stratified Propensity Score Approach},
  volume = {63},
  issn = {00359254},
  shorttitle = {Generalizing from Unrepresentative Experiments},
  doi = {10.1111/rssc.12037},
  language = {en},
  number = {2},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  author = {Colm O'Muircheartaigh and Larry V. Hedges},
  month = {feb},
  year = {2014},
  pages = {195-210},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\NLQC3ENV\\O'Muircheartaigh and Hedges - 2014 - Generalizing from unrepresentative experiments a .pdf},
}
@Article{kernAssessingMethodsGeneralizing2016,
  title = {Assessing {{Methods}} for {{Generalizing Experimental Impact Estimates}} to {{Target Populations}}},
  volume = {9},
  issn = {1934-5747},
  doi = {10.1080/19345747.2015.1060282},
  abstract = {Randomized experiments are considered the gold standard for causal inference because they can provide unbiased estimates of treatment effects for the experimental participants. However, researchers and policymakers are often interested in using a specific experiment to inform decisions about other target populations. In education research, increasing attention is being paid to the potential lack of generalizability of randomized experiments because the experimental participants may be unrepresentative of the target population of interest. This article examines whether generalization may be assisted by statistical methods that adjust for observed differences between the experimental participants and members of a target population. The methods examined include approaches that reweight the experimental data so that participants more closely resemble the target population and methods that utilize models of the outcome. Two simulation studies and one empirical analysis investigate and compare the methods' performance. One simulation uses purely simulated data while the other utilizes data from an evaluation of a school-based dropout prevention program. Our simulations suggest that machine learning methods outperform regression-based methods when the required structural (ignorability) assumptions are satisfied. When these assumptions are violated, all of the methods examined perform poorly. Our empirical analysis uses data from a multisite experiment to assess how well results from a given site predict impacts in other sites. Using a variety of extrapolation methods, predicted effects for each site are compared to actual benchmarks. Flexible modeling approaches perform best, although linear regression is not far behind. Taken together, these results suggest that flexible modeling techniques can aid generalization while underscoring the fact that even state-of-the-art statistical techniques still rely on strong assumptions.},
  number = {1},
  journal = {Journal of Research on Educational Effectiveness},
  author = {Holger L. Kern and Elizabeth A. Stuart and Jennifer Hill and Donald P. Green},
  month = {jan},
  year = {2016},
  keywords = {Bayesian Additive Regression Trees external validity generalizability propensity score weighting},
  pages = {103-127},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\UXJ39Z7E\\Kern et al. - 2016 - Assessing Methods for Generalizing Experimental Im.pdf;C:\\Users\\Gleb\\Zotero\\storage\\GR2Q56U5\\19345747.2015.html},
  pmid = {27668031},
}
@Manual{R-colorblindr,
  title = {colorblindr: Simulate colorblindness in R figures},
  author = {Claire D. McWhite and Claus O. Wilke},
  year = {2019},
  note = {R package version 0.1.0},
  url = {https://github.com/clauswilke/colorblindr},
}
@Article{R-colorspace_a,
  title = {Escaping {RGB}land: Selecting Colors for Statistical Graphics},
  author = {Achim Zeileis and Kurt Hornik and Paul Murrell},
  journal = {Computational Statistics \& Data Analysis},
  year = {2009},
  volume = {53},
  number = {9},
  pages = {3259--3270},
  doi = {10.1016/j.csda.2008.11.033},
}
@Article{R-colorspace_b,
  title = {Somewhere over the Rainbow: How to Make Effective Use of Colors in Meteorological Visualizations},
  author = {Reto Stauffer and Georg J. Mayr and Markus Dabernig and Achim Zeileis},
  journal = {Bulletin of the American Meteorological Society},
  year = {2009},
  volume = {96},
  number = {2},
  pages = {203--216},
  doi = {10.1175/BAMS-D-13-00155.1},
}
@Article{R-ICC,
  title = {Guidelines for Estimating Repeatability},
  author = {Matthew E. Wolak and Daphne J. Fairbairn and Yale R. Paulsen},
  year = {2012},
  journal = {Methods in Ecology and Evolution 3(1):129-137},
}
@Manual{R-tableone,
  title = {tableone: Create 'Table 1' to Describe Baseline Characteristics},
  author = {Kazuki Yoshida},
  year = {2019},
  note = {R package version 0.10.0},
  url = {https://CRAN.R-project.org/package=tableone},
}
@Article{steinleyKmeansClusteringHalfcentury2006,
  title = {K-Means Clustering: {{A}} Half-Century Synthesis},
  volume = {59},
  copyright = {2006 The British Psychological Society},
  issn = {2044-8317},
  shorttitle = {K-Means Clustering},
  abstract = {This paper synthesizes the results, methodology, and research conducted concerning the K-means clustering method over the last fifty years. The K-means method is first introduced, various formulations of the minimum variance loss function and alternative loss functions within the same class are outlined, and different methods of choosing the number of clusters and initialization, variable preprocessing, and data reduction schemes are discussed. Theoretic statistical results are provided and various extensions of K-means using different metrics or modifications of the original algorithm are given, leading to a unifying treatment of K-means and some of its extensions. Finally, several future studies are outlined that could enhance the understanding of numerous subtleties affecting the performance of the K-means method.},
  language = {en},
  number = {1},
  journal = {British Journal of Mathematical and Statistical Psychology},
  doi = {10.1348/000711005X48266},
  author = {Douglas Steinley},
  month = {may},
  year = {2006},
  pages = {1-34},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\LSAJD8WP\\Steinley - 2006 - K-means clustering A half-century synthesis.pdf;C:\\Users\\gf4929\\Zotero\\storage\\CVTEN9SM\\000711005X48266.html},
}
@Article{calinskiDendriteMethodCluster1974,
  title = {A Dendrite Method for Cluster Analysis},
  volume = {3},
  issn = {0090-3272},
  abstract = {A method for identifying clusters of points in a multidimensional Euclidean space is described and its application to taxonomy considered. It reconciles, in a sense, two different approaches to the investigation of the spatial relationships between the points, viz., the agglomerative and the divisive methods. A graph, the shortest dendrite of Florek etal. (1951a), is constructed on a nearest neighbour basis and then divided into clusters by applying the criterion of minimum within cluster sum of squares. This procedure ensures an effective reduction of the number of possible splits. The method may be applied to a dichotomous division, but is perfectly suitable also for a global division into any number of clusters. An informal indicator of the {"}best number{"} of clusters is suggested. It is a{"}variance ratio criterion{"} giving some insight into the structure of the points. The method is illustrated by three examples, one of which is original. The results obtained by the dendrite method are compared with those obtained by using the agglomerative method or Ward (1963) and the divisive method of Edwards and Cavalli-Sforza (1965).},
  number = {1},
  journal = {Communications in Statistics},
  doi = {10.1080/03610927408827101},
  author = {T. Cali{\a'n}ski and J. Harabasz},
  month = {jan},
  year = {1974},
  keywords = {approximate grouping procedure,cluster analysis,minimum variance (WGSS) criterion for optimal grouping,numerical taxonomy,shortest dendrite = minimum spanning tree,variance ratio criterion for best number of groups},
  pages = {1-27},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\I7H43QVU\\CaliÅ„ski and Harabasz - 1974 - A dendrite method for cluster analysis.pdf;C:\\Users\\gf4929\\Zotero\\storage\\EWRRXE4N\\03610927408827101.html},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Manual{R-cowplot,
  title = {cowplot: Streamlined Plot Theme and Plot Annotations for 'ggplot2'},
  author = {Claus O. Wilke},
  year = {2019},
  note = {R package version 1.0.0},
  url = {https://CRAN.R-project.org/package=cowplot},
}
@Manual{R-ggalt,
  title = {ggalt: Extra Coordinate Systems, 'Geoms', Statistical Transformations,
Scales and Fonts for 'ggplot2'},
  author = {Bob Rudis and Ben Bolker and Jan Schulz},
  year = {2017},
  note = {R package version 0.4.0},
  url = {https://CRAN.R-project.org/package=ggalt},
}
@Manual{R-ggrepel,
  title = {ggrepel: Automatically Position Non-Overlapping Text Labels with
'ggplot2'},
  author = {Kamil Slowikowski},
  year = {2019},
  note = {R package version 0.8.1},
  url = {https://CRAN.R-project.org/package=ggrepel},
}
@Manual{R-shiny,
  title = {shiny: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
  year = {2019},
  note = {R package version 1.3.2},
  url = {https://CRAN.R-project.org/package=shiny},
}
@Book{gerberFieldExperimentsDesign2012,
  address = {New York},
  edition = {1st ed},
  title = {Field Experiments: Design, Analysis, and Interpretation},
  isbn = {978-0-393-97995-4},
  lccn = {JA86 .G36 2012},
  shorttitle = {Field Experiments},
  publisher = {{W. W. Norton}},
  author = {Alan S. Gerber and Donald P. Green},
  year = {2012},
  keywords = {Political science,Research Methodology,Social sciences,Study and teaching (Higher)},
  note = {OCLC: 759908573},
}
@Manual{R-cluster,
  title = {cluster: Cluster Analysis Basics and Extensions},
  author = {Martin Maechler and Peter Rousseeuw and Anja Struyf and Mia Hubert and Kurt Hornik},
  year = {2018},
  note = {R package version 2.0.7-1 --- For new features, see the 'Changelog' file (in the package source)},
}
@Manual{R-fpc,
  title = {fpc: Flexible Procedures for Clustering},
  author = {Christian Hennig},
  year = {2019},
  note = {R package version 2.2-3},
  url = {https://CRAN.R-project.org/package=fpc},
}
@Article{tiptonImprovedGeneralizabilityImproved2019,
  title = {Improved {{Generalizability Through Improved Recruitment}}: {{Lessons Learned From}} a {{Large}}-{{Scale Randomized Trial}}},
  volume = {40},
  issn = {1098-2140},
  shorttitle = {Improved {{Generalizability Through Improved Recruitment}}},
  abstract = {Randomized control trials (RCTs) have long been considered the ``gold standard'' for evaluating the impacts of interventions. However, in most education RCTs, the sample of schools included is recruited based on convenience, potentially compromising a study's ability to generalize to an intended population. An alternative approach is to recruit schools using a stratified recruitment method developed by Tipton. Until now, however, there has been limited information available about how to implement this approach in the field. In this article, we concretely illustrate each step of the stratified recruitment method in an evaluation of a college-level developmental algebra intervention. We reflect on the implementation of this process and conclude with five on-the-ground lessons regarding how to best implement this recruitment method in future studies.},
  language = {en},
  number = {3},
  journal = {American Journal of Evaluation},
  doi = {10.1177/1098214018810519},
  author = {Elizabeth Tipton and Bryan J. Matlen},
  month = {sep},
  year = {2019},
  pages = {414-430},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\YXZC5C83\\Tipton and Matlen - 2019 - Improved Generalizability Through Improved Recruit.pdf},
}

@Misc{roschelleRecruitingParticipantsLargeScale,
  title = {Recruiting {{Participants}} for {{Large}}-{{Scale Random Assignment Experiments}} in {{School Settings}}},
  abstract = {Recruitment is a key challenge for researchers conducting any large school-based study, especially random assignment studies where researchers require certain level of control over experimental conditions and intervention implementation. Such studies often need to have a sample of 30-60 teachers or schools to have sufficient power to detect medium-sized effects of interventions. We report here on our experiences in recruiting participants for random assignment experiments in public primary and secondary schools. Our perspective is based on over twenty current and completed randomized controlled trials (RCT) in K\textendash{}12 school settings conducted by SRI International, including studies of educational technology, literacy, mathematics, science, instructional materials, teacher professional development, and student behavioral supports, following What Works Clearinghouse standards for RCTs. Considering our experience across these studies, we reflect on how we approached the recruitment problem and what worked during our efforts. Our comments are organized in six topics corresponding to the aspects of an overall recruitment process: 1) Study design; 2) Intervention packaging; 3) Planning a recruitment process; 4) Designing recruitment messages; 5) Running a recruitment campaign; 6) After recruitment.},
  howpublished = {https://www.sri.com/work/publications/recruiting-participants-large-scale-random-assignment-experiments-school-settings},
  author = {Jeremy Roschelle and Mingyu Feng and H. Alix Gallagher and Robert Murphy and Christopher Harris and Danae Kamdar and Gucci Trinidad},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\6EJY8LAL\\Roschelle et al. - Recruiting Participants for Large-Scale Random Ass.pdf;C:\\Users\\gf4929\\Zotero\\storage\\9GDL4C7T\\recruiting-participants-large-scale-random-assignment-experiments-school-settings.html},
}

@Article{stuartGeneralizingTreatmentEffect2017,
  title = {Generalizing {{Treatment Effect Estimates From Sample}} to {{Population}}: {{A Case Study}} in the {{Difficulties}} of {{Finding Sufficient Data}}},
  volume = {41},
  issn = {0193-841X},
  shorttitle = {Generalizing {{Treatment Effect Estimates From Sample}} to {{Population}}},
  abstract = {Background:Given increasing concerns about the relevance of research to policy and practice, there is growing interest in assessing and enhancing the external validity of randomized trials: determining how useful a given randomized trial is for informing a policy question for a specific target population.Objectives:This article highlights recent advances in assessing and enhancing external validity, with a focus on the data needed to make ex post statistical adjustments to enhance the applicability of experimental findings to populations potentially different from their study sample.Research design:We use a case study to illustrate how to generalize treatment effect estimates from a randomized trial sample to a target population, in particular comparing the sample of children in a randomized trial of a supplemental program for Head Start centers (the Research-Based, Developmentally Informed study) to the national population of children eligible for Head Start, as represented in the Head Start Impact Study.Results:For this case study, common data elements between the trial sample and population were limited, making reliable generalization from the trial sample to the population challenging.Conclusions:To answer important questions about external validity, more publicly available data are needed. In addition, future studies should make an effort to collect measures similar to those in other data sets. Measure comparability between population data sets and randomized trials that use samples of convenience will greatly enhance the range of research and policy relevant questions that can be answered.},
  language = {en},
  number = {4},
  journal = {Evaluation Review},
  doi = {10.1177/0193841X16660663},
  author = {Elizabeth A. Stuart and Anna Rhodes},
  month = {aug},
  year = {2017},
  pages = {357-388},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\WX9ZNYJK\\Stuart and Rhodes - 2017 - Generalizing Treatment Effect Estimates From Sampl.pdf},
}
@Manual{R-concaveman,
  title = {concaveman: A Very Fast 2D Concave Hull Algorithm},
  author = {Joël Gombin and Ramnath Vaidyanathan and Vladimir Agafonkin},
  year = {2020},
  note = {R package version 1.1.0},
  url = {https://CRAN.R-project.org/package=concaveman},
}
@Manual{R-ggforce,
  title = {ggforce: Accelerating 'ggplot2'},
  author = {Thomas Lin Pedersen},
  year = {2020},
  note = {R package version 0.3.2},
  url = {https://CRAN.R-project.org/package=ggforce},
}
@Manual{R-future,
  title = {future: Unified Parallel and Distributed Processing in R for Everyone},
  author = {Henrik Bengtsson},
  year = {2020},
  note = {R package version 1.18.0},
  url = {https://CRAN.R-project.org/package=future},
}
@Article{imaiMisunderstandingsExperimentalistsObservationalists2008,
  title = {Misunderstandings between Experimentalists and Observationalists about Causal Inference},
  author = {Kosuke Imai and Gary King and Elizabeth A. Stuart},
  year = {2008},
  month = {apr},
  volume = {171},
  pages = {481--502},
  issn = {1467-985X},
  doi = {10.1111/j.1467-985X.2007.00527.x},
  abstract = {Summary. We attempt to clarify, and suggest how to avoid, several serious misunderstandings about and fallacies of causal inference. These issues concern some of the most fundamental advantages and disadvantages of each basic research design. Problems include improper use of hypothesis tests for covariate balance between the treated and control groups, and the consequences of using randomization, blocking before randomization and matching after assignment of treatment to achieve covariate balance. Applied researchers in a wide range of scientific disciplines seem to fall prey to one or more of these fallacies and as a result make suboptimal design or analysis choices. To clarify these points, we derive a new four-part decomposition of the key estimation errors in making causal inferences. We then show how this decomposition can help scholars from different experimental and observational research traditions to understand better each other's inferential problems and attempted solutions.},
  copyright = {\textcopyright{} 2008 Royal Statistical Society},
  file = {C\:\\Users\\Gleb\\Zotero\\storage\\JJZ8M4RX\\Imai et al. - 2008 - Misunderstandings between experimentalists and obs.pdf;C\:\\Users\\Gleb\\Zotero\\storage\\QTQPHUKF\\j.1467-985X.2007.00527.html},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  keywords = {Average treatment effects,Blocking,Covariate balance,Matching,Observational studies,Randomized experiments},
  language = {en},
  number = {2},
}
@Article{tiptonSystemEvidenceAll2020,
  title = {Toward a {{System}} of {{Evidence}} for {{All}}: {{Current Practices}} and {{Future Opportunities}} in 37 {{Randomized Trials}}},
  shorttitle = {Toward a {{System}} of {{Evidence}} for {{All}}},
  author = {Elizabeth Tipton and Jessaca Spybrook and Kaitlyn G. Fitzgerald and Qian Wang and Caryn Davidson},
  year = {2020},
  month = {sep},
  pages = {0013189X20960686},
  publisher = {{American Educational Research Association}},
  issn = {0013-189X},
  doi = {10.3102/0013189X20960686},
  abstract = {As a result of the evidence-based decision-making movement, the number of randomized trials evaluating educational programs and curricula has increased dramatically over the past 20 years. Policy makers and practitioners are encouraged to use the results of these trials to inform their decision making in schools and school districts. At the same time, however, little is known about the schools taking part in these randomized trials, both regarding how and why they were recruited and how they compare to populations in need of research. In this article, we report on a study of 37 cluster randomized trials funded by the Institute of Education Sciences between 2011 and 2015. Principal investigators of these grants were interviewed regarding the recruitment process and practices. Additionally, data on the schools included in 34 of these studies were analyzed to determine the general demographics of schools included in funded research, as well as how these samples compare to important policy relevant populations. We show that the types of schools included in research differ in a variety of ways from these populations. Large schools from large school districts in urban areas were overrepresented, whereas schools from small school districts in rural areas and towns are underrepresented. The article concludes with a discussion of how recruitment practices might be improved in order to meet the goals of the evidence-based decision-making movement.},
  file = {C\:\\Users\\Gleb\\Zotero\\storage\\EHMLHNAP\\Tipton et al. - 2020 - Toward a System of Evidence for All Current Pract.pdf},
  journal = {Educational Researcher},
  language = {en},
}
@Book{lohrSamplingDesignAnalysis2019,
  title = {Sampling: {{Design}} and {{Analysis}}},
  shorttitle = {Sampling},
  author = {Sharon L. Lohr},
  year = {2019},
  month = {apr},
  edition = {2nd Edition},
  publisher = {{Chapman and Hall/CRC}},
  abstract = {This edition is a reprint of the second edition published by Cengage Learning, Inc. Reprinted with permission. What is the unemployment rate? How many adults have high blood pressure? What is the total area of land planted with soybeans? Sampling: Design and Analysis tells you how to design and analyze surveys to answer these and other questions. This authoritative text, used as a standard reference by numerous survey organizations, teaches sampling using real data sets from social sciences, public opinion research, medicine, public health, economics, agriculture, ecology, and other fields. The book is accessible to students from a wide range of statistical backgrounds. By appropriate choice of sections, it can be used for a graduate class for statistics students or for a class with students from business, sociology, psychology, or biology. Readers should be familiar with concepts from an introductory statistics class including linear regression; optional sections contain the statistical theory, for readers who have studied mathematical statistics. Distinctive features include:   More than 450 exercises. In each chapter, Introductory Exercises develop skills, Working with Data Exercises give practice with data from surveys, Working with Theory Exercises allow students to investigate statistical properties of estimators, and Projects and Activities Exercises integrate concepts. A solutions manual is available.   An emphasis on survey design.   Coverage of simple random, stratified, and cluster sampling; ratio estimation; constructing survey weights; jackknife and bootstrap; nonresponse; chi-squared tests and regression analysis.   Graphing data from surveys.   Computer code using SAS\textregistered{} software.   Online supplements containing data sets, computer programs, and additional material.  Sharon Lohr, the author of Measuring Crime: Behind the Statistics, has published widely about survey sampling and statistical methods for education, public policy, law, and crime. She has been recognized as Fellow of the American Statistical Association, elected member of the International Statistical Institute, and recipient of the Gertrude M. Cox Statistics Award and the Deming Lecturer Award. Formerly Dean's Distinguished Professor of Statistics at Arizona State University and a Vice President at Westat, she is now a freelance statistical consultant and writer. Visit her website at www.sharonlohr.com.},
  isbn = {978-0-367-27341-5},
  language = {English},
}
@Book{machlerClusterClusterAnalysis2012,
  title = {Cluster: {{Cluster Analysis Basics}} and {{Extensions}}},
  shorttitle = {Cluster},
  author = {Martin M{\"a}chler and Peter Rousseeuw and Anja Struyf and Mia Hubert and Kurt Hornik},
  year = {2012},
  month = {jan},
  volume = {1},
  abstract = {R package, available on CRAN},
  file = {C\:\\Users\\Gleb\\Zotero\\storage\\XBX5F9X8\\MÃ¤chler et al. - 2012 - Cluster Cluster Analysis Basics and Extensions.pdf},
}
@Misc{reardonseanf.StanfordEducationData2018,
  title = {Stanford {{Education Data Archive}}},
  author = {Sean F. Reardon and Andrew D. Ho and Benjamin R. Shear and Erin M. Fahle and Demetra Kalogrides},
  year = {2018},
}
