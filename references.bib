@Article{tiptonImprovingGeneralizationsExperiments2013,
  title = {Improving {{Generalizations From Experiments Using Propensity Score Subclassification}}: {{Assumptions}}, {{Properties}}, and {{Contexts}}},
  volume = {38},
  issn = {1076-9986},
  url = {https://www.jstor.org/stable/41999424},
  shorttitle = {Improving {{Generalizations From Experiments Using Propensity Score Subclassification}}},
  abstract = {As a result of the use of random assignment to treatment, randomized experiments typically have high internal validity. However, units are very rarely randomly selected from a well-defined population of interest into an experiment; this results in low external validity. Under nonrandom sampling, this means that the estimate of the sample average treatment effect calculated in the experiment can be a biased estimate of the population average treatment effect. This article explores the use of the propensity score subclassification estimator as a means for improving generalizations from experiments. It first lays out the assumptions necessary for generalizations, then investigates the amount of bias reduction and average variance inflation that is likely when compared to a conventional estimator. It concludes with a discussion of issues that arise when the population of interest is not well represented by the experiment, and an example.},
  number = {3},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  urldate = {2018-12-05},
  date = {2013},
  pages = {239-266},
  author = {Elizabeth Tipton},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\62FUAEDQ\\Tipton - 2013 - Improving Generalizations From Experiments Using P.pdf},
}

@Article{tiptonStratifiedSamplingUsing2013,
  langid = {english},
  title = {Stratified {{Sampling Using Cluster Analysis}}: {{A Sample Selection Strategy}} for {{Improved Generalizations From Experiments}}},
  volume = {37},
  issn = {0193-841X},
  url = {https://doi.org/10.1177/0193841X13516324},
  doi = {10.1177/0193841X13516324},
  shorttitle = {Stratified {{Sampling Using Cluster Analysis}}},
  abstract = {Background:An important question in the design of experiments is how to ensure that the findings from the experiment are generalizable to a larger population. This concern with generalizability is particularly important when treatment effects are heterogeneous and when selecting units into the experiment using random sampling is not possible?two conditions commonly met in large-scale educational experiments.Method:This article introduces a model-based balanced-sampling framework for improving generalizations, with a focus on developing methods that are robust to model misspecification. Additionally, the article provides a new method for sample selection within this framework: First units in an inference population are divided into relatively homogenous strata using cluster analysis, and then the sample is selected using distance rankings.Result:In order to demonstrate and evaluate the method, a reanalysis of a completed experiment is conducted. This example compares samples selected using the new method with the actual sample used in the experiment. Results indicate that even under high nonresponse, balance is better on most covariates and that fewer coverage errors result.Conclusion:The article concludes with a discussion of additional benefits and limitations of the method.},
  number = {2},
  journaltitle = {Evaluation Review},
  shortjournal = {Eval Rev},
  urldate = {2018-12-05},
  date = {2013-04-01},
  pages = {109-139},
  author = {Elizabeth Tipton},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\UBJEYKFR\\Tipton - 2013 - Stratified Sampling Using Cluster Analysis A Samp.pdf},
}
@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2018},
  url = {https://www.R-project.org/},
}
@Manual{R-bindrcpp,
  title = {bindrcpp: An 'Rcpp' Interface to Active Bindings},
  author = {Kirill Müller},
  year = {2017},
  note = {R package version 0.2},
  url = {https://CRAN.R-project.org/package=bindrcpp},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain Francois and Lionel Henry and Kirill Müller},
  year = {2017},
  note = {R package version 0.7.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-forcats,
  title = {forcats: Tools for Working with Categorical Variables (Factors)},
  author = {Hadley Wickham},
  year = {2018},
  note = {R package version 0.3.0},
  url = {https://CRAN.R-project.org/package=forcats},
}
@Article{R-Formula,
  title = {Extended Model Formulas in {R}: Multiple Parts and Multiple Responses},
  author = {Achim Zeileis and Yves Croissant},
  journal = {Journal of Statistical Software},
  year = {2010},
  volume = {34},
  number = {1},
  pages = {1--13},
  doi = {10.18637/jss.v034.i01},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2009},
  isbn = {978-0-387-98140-6},
  url = {http://ggplot2.org},
}
@Manual{R-Hmisc,
  title = {Hmisc: Harrell Miscellaneous},
  author = {Frank E {Harrell Jr} and with contributions from Charles Dupont and many others.},
  year = {2018},
  note = {R package version 4.1-1},
  url = {https://CRAN.R-project.org/package=Hmisc},
}
@Manual{R-htmlTable,
  title = {htmlTable: Advanced Tables for Markdown/HTML},
  author = {Max Gordon and Stephen Gragg and Peter Konings},
  year = {2018},
  note = {R package version 1.11.2},
  url = {https://CRAN.R-project.org/package=htmlTable},
}
@Book{R-lattice,
  title = {Lattice: Multivariate Data Visualization with R},
  author = {Deepayan Sarkar},
  publisher = {Springer},
  address = {New York},
  year = {2008},
  note = {ISBN 978-0-387-75968-5},
  url = {http://lmdvr.r-forge.r-project.org},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2018},
  note = {R package version 0.1.0.9842},
  url = {https://github.com/crsh/papaja},
}
@Manual{R-purrr,
  title = {purrr: Functional Programming Tools},
  author = {Lionel Henry and Hadley Wickham},
  year = {2017},
  note = {R package version 0.2.4},
  url = {https://CRAN.R-project.org/package=purrr},
}
@Manual{R-readr,
  title = {readr: Read Rectangular Text Data},
  author = {Hadley Wickham and Jim Hester and Romain Francois},
  year = {2017},
  note = {R package version 1.1.1},
  url = {https://CRAN.R-project.org/package=readr},
}
@Manual{R-stargazer,
  title = {stargazer: Well-Formatted Regression and Summary Statistics Tables},
  author = {Marek Hlavac},
  year = {2018},
  note = {R package version 5.2.1},
  organization = {Central European Labour Studies Institute (CELSI)},
  address = {Bratislava, Slovakia},
  url = {https://CRAN.R-project.org/package=stargazer},
}
@Manual{R-stringr,
  title = {stringr: Simple, Consistent Wrappers for Common String Operations},
  author = {Hadley Wickham},
  year = {2018},
  note = {R package version 1.3.0},
  url = {https://CRAN.R-project.org/package=stringr},
}
@Book{R-survival-book,
  title = {Modeling Survival Data: Extending the {C}ox Model},
  author = {{Terry M. Therneau} and {Patricia M. Grambsch}},
  year = {2000},
  publisher = {Springer},
  address = {New York},
  isbn = {0-387-98784-3},
}
@Manual{R-tibble,
  title = {tibble: Simple Data Frames},
  author = {Kirill Müller and Hadley Wickham},
  year = {2018},
  note = {R package version 1.4.2},
  url = {https://CRAN.R-project.org/package=tibble},
}
@Manual{R-tidyr,
  title = {tidyr: Easily Tidy Data with 'spread()' and 'gather()' Functions},
  author = {Hadley Wickham and Lionel Henry},
  year = {2018},
  note = {R package version 0.8.0},
  url = {https://CRAN.R-project.org/package=tidyr},
}
@Manual{R-tidyverse,
  title = {tidyverse: Easily Install and Load the 'Tidyverse'},
  author = {Hadley Wickham},
  year = {2017},
  note = {R package version 1.2.1},
  url = {https://CRAN.R-project.org/package=tidyverse},
}
@Manual{R-xtable,
  title = {xtable: Export Tables to LaTeX or HTML},
  author = {David B. Dahl},
  year = {2016},
  note = {R package version 1.8-2},
  url = {https://CRAN.R-project.org/package=xtable},
}
@Article{stuartUsePropensityScores2011,
  langid = {english},
  title = {The Use of Propensity Scores to Assess the Generalizability of Results from Randomized Trials: {{Use}} of {{Propensity Scores}} to {{Assess Generalizability}}},
  volume = {174},
  issn = {09641998},
  url = {http://doi.wiley.com/10.1111/j.1467-985X.2010.00673.x},
  doi = {10.1111/j.1467-985X.2010.00673.x},
  shorttitle = {The Use of Propensity Scores to Assess the Generalizability of Results from Randomized Trials},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  urldate = {2018-12-05},
  date = {2011-04},
  pages = {369-386},
  author = {Elizabeth A. Stuart and Stephen R. Cole and Catherine P. Bradshaw and Philip J. Leaf},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\Z2VSWX4P\\Stuart et al. - 2011 - The use of propensity scores to assess the general.pdf},
}
@Book{shadishExperimentalQuasiexperimentalDesigns2002,
  address = {Boston, MA, US},
  series = {Experimental and quasi-experimental designs for generalized causal inference},
  title = {Experimental and Quasi-Experimental Designs for Generalized Causal Inference},
  isbn = {978-0-395-61556-0},
  abstract = {This is a book for those who have already decided that identifying a dependable relationship between a cause and its effects is a high priority and who wish to consider experimental methods for doing so. Such causal relationships are of great importance in human affairs. The rewards associated with being correct in identifying causal relationships can be high, an the costs of misidentification can be tremendous. This book has two major purposes: to describe ways in which testing causal propositions can be improved in specific research projects, and to describe ways to improve generalizations about causal propositions. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  publisher = {{Houghton, Mifflin and Company}},
  author = {William R. Shadish and Thomas D. Cook and Donald T. Campbell},
  year = {2002},
  keywords = {Causal Analysis,Experimental Design,Experimental Methods,Quasi Experimental Methods},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\MDZLE8PQ\\2002-17373-000.html},
}
@Article{olsenExternalValidityPolicy2013,
  title = {External {{Validity}} in {{Policy Evaluations That Choose Sites Purposively}}},
  volume = {32},
  copyright = {\textcopyright{} 2012 by the Association for Public Policy Analysis and Management},
  issn = {1520-6688},
  doi = {10.1002/pam.21660},
  abstract = {Evaluations of the impact of social programs are often carried out in multiple sites, such as school districts, housing authorities, local TANF offices, or One-Stop Career Centers. Most evaluations select sites purposively following a process that is nonrandom. Unfortunately, purposive site selection can produce a sample of sites that is not representative of the population of interest for the program. In this paper, we propose a conceptual model of purposive site selection. We begin with the proposition that a purposive sample of sites can usefully be conceptualized as a random sample of sites from some well-defined population, for which the sampling probabilities are unknown and vary across sites. This proposition allows us to derive a formal, yet intuitive, mathematical expression for the bias in the pooled impact estimate when sites are selected purposively. This formula helps us to better understand the consequences of selecting sites purposively, and the factors that contribute to the bias. Additional research is needed to obtain evidence on how large the bias tends to be in actual studies that select sites purposively, and to develop methods to increase the external validity of these studies. \textcopyright{} 2012 by the Association for Public Policy Analysis and Management.},
  language = {en},
  number = {1},
  journal = {Journal of Policy Analysis and Management},
  author = {Robert B. Olsen and Larry L. Orr and Stephen H. Bell and Elizabeth A. Stuart},
  year = {2013},
  pages = {107-121},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\A4IXDTZY\\Olsen et al. - 2013 - External Validity in Policy Evaluations That Choos.pdf;C:\\Users\\Gleb\\Zotero\\storage\\TJFGEVLH\\pam.html},
}
@Article{tiptonHowGeneralizableYour2014,
  title = {How {{Generalizable Is Your Experiment}}? {{An Index}} for {{Comparing Experimental Samples}} and {{Populations}}},
  volume = {39},
  issn = {1076-9986},
  shorttitle = {How {{Generalizable Is Your Experiment}}?},
  abstract = {Although a large-scale experiment can provide an estimate of the average causal impact for a program, the sample of sites included in the experiment is often not drawn randomly from the inference population of interest. In this article, we provide a generalizability index that can be used to assess the degree of similarity between the sample of units in an experiment and one or more inference populations on a set of selected covariates. The index takes values between 0 and 1 and indicates both when a sample is like a miniature of the population and how well reweighting methods may perform when differences exist. Results of simulation studies are provided that develop rules of thumb for interpretation as well as an example.},
  number = {6},
  journal = {Journal of Educational and Behavioral Statistics},
  author = {Elizabeth Tipton},
  year = {2014},
  pages = {478-501},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\C929YILU\\Tipton - 2014 - How Generalizable Is Your Experiment An Index for.pdf},
}
@Thesis{fellersDevelopingApproachDetermine2017,
  langid = {english},
  location = {{United States -- New York}},
  title = {Developing an Approach to Determine Generalizability: {{A}} Review of Efficacy and Effectiveness Trials Funded by the {{Institute}} of {{Education Sciences}}},
  url = {https://search.proquest.com/docview/1865595768/abstract/40FD82F4A0C24535PQ/1},
  shorttitle = {Developing an Approach to Determine Generalizability},
  abstract = {Since its establishment the Institute of Education Sciences has been creating opportunities and driving standards to generate research in education that is high quality rigorous, and relevant. This dissertation is an analysis of current practices in Goal III and Goal IV studies, in order to (1) better understand of the types of schools that agree to take part in these studies, and (2) an assess how representative these schools are in comparison to important policy relevant populations. This dissertation focuses on a subset of studies that were funded from 2005-2014 by the Department of Education, IES, under the NCER grants-funding arm. Studies included were those whose interventions were aimed at elementary students across core curriculum and ELL program areas. Study schools were compared to two main populations, the U.S population of elementary schools and Title I elementary schools, as well as these populations on a state level. The B-index, proposed by Tipton (2014) was the main value of comparison used to assess the compositional similarity, or generalizability, of study schools to these identified inference populations. The findings show that across all studies included in this analysis, participating schools were representative of the U.S. population of schools, B-index = 0.9. Comparisons were also made between this collection of schools and the respective populations at the state level. Results showed that these schools were not representative of any individual states (no B-index values were greater than 0.90). Across all included studies, schools that agreed to participate were more often located in urban areas, had higher rates of FRL students, had more minority students enrolled, and had more total students, in both district and school, than those schools in the population of U.S. schools. It is clear that the movement of education research is to be relevant to a larger audience. Through this study it is clear that, across studies, we are achieving some representation in IES funded studies. However, the finer comparisons, study samples to individual state and individual studies to these populations, show limited similarity between study schools and populations of interest to policy makers using these study findings to make decisions about their schools.},
  pagetotal = {130},
  institution = {{Columbia University}},
  type = {Ph.D.},
  urldate = {2018-12-04},
  date = {2017},
  keywords = {Education,External validity,Generalizability,Pure sciences,Recruitment},
  author = {Lauren Fellers},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\VH9IMSKE\\Fellers - 2017 - Developing an approach to determine generalizabili.pdf},
}

@Article{tiptonSiteSelectionExperiments2016,
  title = {Site {{Selection}} in {{Experiments}}: {{An Assessment}} of {{Site Recruitment}} and {{Generalizability}} in {{Two Scale}}-up {{Studies}}},
  volume = {9},
  issn = {1934-5747},
  url = {https://doi.org/10.1080/19345747.2015.1105895},
  doi = {10.1080/19345747.2015.1105895},
  shorttitle = {Site {{Selection}} in {{Experiments}}},
  abstract = {Recently, statisticians have begun developing methods to improve the generalizability of results from large-scale experiments in education. This work has included the development of methods for improved site selection when random sampling is infeasible, including the use of stratification and targeted recruitment strategies. This article provides the next step in this literature—a template for assessing generalizability after a study is completed. In this template, first records from the recruitment process are analyzed, comparing differences between those who agreed to be in the study and those who did not. Second, the final sample is compared to the original inference population and different possible subsets, with the goal of determining where the results best generalize (and where they do not). Throughout, these methods are situated in the post hoc analysis of results from two scale-up studies. The article ends with a discussion of the use of these methods more generally when reporting results from randomized trials.},
  issue = {sup1},
  journaltitle = {Journal of Research on Educational Effectiveness},
  urldate = {2018-12-05},
  date = {2016-10-03},
  pages = {209-228},
  keywords = {external validity,generalization,recruitment},
  author = {Elizabeth Tipton and Lauren Fellers and Sarah Caverly and Michael Vaden-Kiernan and Geoffrey Borman and Kate Sullivan and Veronica Ruiz {de Castilla}},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\YCNS43HB\\Tipton et al. - 2016 - Site Selection in Experiments An Assessment of Si.pdf;C:\\Users\\gf4929\\Zotero\\storage\\9B5MP5FJ\\19345747.2015.html},
}

@Article{stuartCharacteristicsSchoolDistricts2017,
  title = {Characteristics of {{School Districts That Participate}} in {{Rigorous National Educational Evaluations}}},
  volume = {10},
  issn = {1934-5747},
  url = {https://doi.org/10.1080/19345747.2016.1205160},
  doi = {10.1080/19345747.2016.1205160},
  abstract = {Given increasing interest in evidence-based policy, there is growing attention to how well the results from rigorous program evaluations may inform policy decisions. However, little attention has been paid to documenting the characteristics of schools or districts that participate in rigorous educational evaluations, and how they compare to potential target populations for the interventions that were evaluated. Utilizing a list of the actual districts that participated in 11 large-scale rigorous educational evaluations, we compare those districts to several different target populations of districts that could potentially be affected by policy decisions regarding the interventions under study. We find that school districts that participated in the 11 rigorous educational evaluations differ from the interventions' target populations in several ways, including size, student performance on state assessments, and location (urban/rural). These findings raise questions about whether, as currently implemented, the results from rigorous impact studies in education are likely to generalize to the larger set of school districts—and thus schools and students—of potential interest to policymakers, and how we can improve our study designs to retain strong internal validity while also enhancing external validity.},
  number = {1},
  journaltitle = {Journal of Research on Educational Effectiveness},
  urldate = {2018-12-19},
  date = {2017-01-02},
  pages = {168-206},
  keywords = {Corrigendum,external validity,generalizability,randomized experiment},
  author = {Elizabeth A. Stuart and Stephen H. Bell and Cyrus Ebnesajjad and Robert B. Olsen and Larry L. Orr},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\7KRDW2UM\\Stuart et al. - 2017 - Characteristics of School Districts That Participa.pdf;C:\\Users\\gf4929\\Zotero\\storage\\EVBPF4PN\\19345747.2016.html},
}
@Article{gowerGeneralCoefficientSimilarity1971,
  title = {A {{General Coefficient}} of {{Similarity}} and {{Some}} of {{Its Properties}}},
  volume = {27},
  issn = {0006-341X},
  doi = {10.2307/2528823},
  abstract = {[A general coefficient measuring the similarity between two sampling units is defined. The matrix of similarities between all pairs of sample units is shown to be positive semidefinite (except possibly when there are missing values). This is important for the multidimensional Euclidean representation of the sample and also establishes some inequalities amongst the similarities relating three individuals. The definition is extended to cope with a hierarchy of characters.]},
  number = {4},
  journal = {Biometrics},
  author = {J. C. Gower},
  year = {1971},
  pages = {857-871},
}
@Article{roschelleIntegrationTechnologyCurriculum2010,
  title = {Integration of {{Technology}}, {{Curriculum}}, and {{Professional Development}} for {{Advancing Middle School Mathematics}}: {{Three Large}}-{{Scale Studies}}},
  volume = {47},
  issn = {0002-8312},
  shorttitle = {Integration of {{Technology}}, {{Curriculum}}, and {{Professional Development}} for {{Advancing Middle School Mathematics}}},
  abstract = {[The authors present three studies (two randomized controlled experiments and one embedded quasi-eocperiment) designed to evaluate the impact of replacement units targeting student learning of advanced middle school mathematics. The studies evaluated the SimCalc approach, which integrates an interactive representational technology, paper curriculum, and teacher professional development. Each study addressed both replicability of findings and robustness across Texas settings, with varied teacher characteristics (backgrounds, knowledge, attitudes) and student characteristics (demographics, levels of prior mathematics knowledge). Analyses revealed statistically significant main effects, with student-level effect sizes of .63, .50, and .56. These consistent gains support the conclusion that SimCalc is effective in enabling a wide variety of teachers in a diversity of settings to extend student learning to more advanced mathematics.]},
  number = {4},
  journal = {American Educational Research Journal},
  author = {Jeremy Roschelle and Nicole Shechtman and Deborah Tatar and Stephen Hegedus and Bill Hopkins and Susan Empson and Jennifer Knudsen and Lawrence P. Gallagher},
  year = {2010},
  pages = {833-878},
}
@Article{tiptonImplicationsSmallSamples2017,
  title = {Implications of {{Small Samples}} for {{Generalization}}: {{Adjustments}} and {{Rules}} of {{Thumb}}},
  volume = {41},
  issn = {0193-841X},
  shorttitle = {Implications of {{Small Samples}} for {{Generalization}}},
  doi = {10.1177/0193841X16655665},
  abstract = {Background:Policy makers and researchers are frequently interested in understanding how effective a particular intervention may be for a specific population. One approach is to assess the degree of similarity between the sample in an experiment and the population. Another approach is to combine information from the experiment and the population to estimate the population average treatment effect (PATE).Method:Several methods for assessing the similarity between a sample and population currently exist as well as methods estimating the PATE. In this article, we investigate properties of six of these methods and statistics in the small sample sizes common in education research (i.e., 10?70 sites), evaluating the utility of rules of thumb developed from observational studies in the generalization case.Result:In small random samples, large differences between the sample and population can arise simply by chance and many of the statistics commonly used in generalization are a function of both sample size and the number of covariates being compared. The rules of thumb developed in observational studies (which are commonly applied in generalization) are much too conservative given the small sample sizes found in generalization.Conclusion:This article implies that sharp inferences to large populations from small experiments are difficult even with probability sampling. Features of random samples should be kept in mind when evaluating the extent to which results from experiments conducted on nonrandom samples might generalize.},
  language = {en},
  number = {5},
  journal = {Evaluation Review},
  author = {Elizabeth Tipton and Kelly Hallberg and Larry V. Hedges and Wendy Chan},
  month = {oct},
  year = {2017},
  pages = {472-505},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\ZULTL9AB\\Tipton et al. - 2017 - Implications of Small Samples for Generalization .pdf},
}
@Article{hennigHowFindAppropriate2013,
  title = {How to Find an Appropriate Clustering for Mixed-Type Variables with Application to Socio-Economic Stratification: {{How}} to {{Find}} an {{Appropriate Clustering}}},
  volume = {62},
  issn = {00359254},
  shorttitle = {How to Find an Appropriate Clustering for Mixed-Type Variables with Application to Socio-Economic Stratification},
  doi = {10.1111/j.1467-9876.2012.01066.x},
  language = {en},
  number = {3},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  author = {Christian Hennig and Tim F. Liao},
  month = {may},
  year = {2013},
  pages = {309-369},
  file = {C:\\Users\\gf4929\\Zotero\\storage\\VRJ6M95F\\Hennig and Liao - 2013 - How to find an appropriate clustering for mixed-ty.pdf},
}
@Manual{R-kableExtra,
  title = {kableExtra: Construct Complex Table with 'kable' and Pipe Syntax},
  author = {Hao Zhu},
  year = {2018},
  note = {R package version 0.9.0},
  url = {https://CRAN.R-project.org/package=kableExtra},
}
@Article{raudenbushStatisticalPowerOptimal2000,
  title = {Statistical Power and Optimal Design for Multisite Randomized Trials.},
  volume = {5},
  issn = {1082-989X},
  doi = {10.1037//1082-989X.5.2.199},
  language = {en},
  number = {2},
  journal = {Psychological Methods},
  author = {Stephen W. Raudenbush and Xiaofeng Liu},
  year = {2000},
  pages = {199-213},
}
@Book{grovesSurveyMethodology2004,
  address = {Hoboken, N.J},
  series = {Wiley Series in Survey Methodology},
  title = {Survey Methodology},
  isbn = {978-0-471-48348-9},
  lccn = {HA31.2 .S873 2004},
  publisher = {{Wiley-Interscience}},
  editor = {Robert M. Groves},
  year = {2004},
  keywords = {Methodology,Research Statistical methods,Social sciences,Social surveys,Surveys},
}
@Article{omuircheartaighGeneralizingUnrepresentativeExperiments2014,
  title = {Generalizing from Unrepresentative Experiments: A Stratified Propensity Score Approach},
  volume = {63},
  issn = {00359254},
  shorttitle = {Generalizing from Unrepresentative Experiments},
  doi = {10.1111/rssc.12037},
  language = {en},
  number = {2},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  author = {Colm O'Muircheartaigh and Larry V. Hedges},
  month = {feb},
  year = {2014},
  pages = {195-210},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\NLQC3ENV\\O'Muircheartaigh and Hedges - 2014 - Generalizing from unrepresentative experiments a .pdf},
}
@Article{kernAssessingMethodsGeneralizing2016,
  title = {Assessing {{Methods}} for {{Generalizing Experimental Impact Estimates}} to {{Target Populations}}},
  volume = {9},
  issn = {1934-5747},
  doi = {10.1080/19345747.2015.1060282},
  abstract = {Randomized experiments are considered the gold standard for causal inference because they can provide unbiased estimates of treatment effects for the experimental participants. However, researchers and policymakers are often interested in using a specific experiment to inform decisions about other target populations. In education research, increasing attention is being paid to the potential lack of generalizability of randomized experiments because the experimental participants may be unrepresentative of the target population of interest. This article examines whether generalization may be assisted by statistical methods that adjust for observed differences between the experimental participants and members of a target population. The methods examined include approaches that reweight the experimental data so that participants more closely resemble the target population and methods that utilize models of the outcome. Two simulation studies and one empirical analysis investigate and compare the methods' performance. One simulation uses purely simulated data while the other utilizes data from an evaluation of a school-based dropout prevention program. Our simulations suggest that machine learning methods outperform regression-based methods when the required structural (ignorability) assumptions are satisfied. When these assumptions are violated, all of the methods examined perform poorly. Our empirical analysis uses data from a multisite experiment to assess how well results from a given site predict impacts in other sites. Using a variety of extrapolation methods, predicted effects for each site are compared to actual benchmarks. Flexible modeling approaches perform best, although linear regression is not far behind. Taken together, these results suggest that flexible modeling techniques can aid generalization while underscoring the fact that even state-of-the-art statistical techniques still rely on strong assumptions.},
  number = {1},
  journal = {Journal of Research on Educational Effectiveness},
  author = {Holger L. Kern and Elizabeth A. Stuart and Jennifer Hill and Donald P. Green},
  month = {jan},
  year = {2016},
  keywords = {Bayesian Additive Regression Trees external validity generalizability propensity score weighting},
  pages = {103-127},
  file = {C:\\Users\\Gleb\\Zotero\\storage\\UXJ39Z7E\\Kern et al. - 2016 - Assessing Methods for Generalizing Experimental Im.pdf;C:\\Users\\Gleb\\Zotero\\storage\\GR2Q56U5\\19345747.2015.html},
  pmid = {27668031},
}
@Manual{R-colorblindr,
  title = {colorblindr: Simulate colorblindness in R figures},
  author = {Claire D. McWhite and Claus O. Wilke},
  year = {2019},
  note = {R package version 0.1.0},
  url = {https://github.com/clauswilke/colorblindr},
}
@Article{R-colorspace_a,
  title = {Escaping {RGB}land: Selecting Colors for Statistical Graphics},
  author = {Achim Zeileis and Kurt Hornik and Paul Murrell},
  journal = {Computational Statistics \& Data Analysis},
  year = {2009},
  volume = {53},
  number = {9},
  pages = {3259--3270},
  doi = {10.1016/j.csda.2008.11.033},
}
@Article{R-colorspace_b,
  title = {Somewhere over the Rainbow: How to Make Effective Use of Colors in Meteorological Visualizations},
  author = {Reto Stauffer and Georg J. Mayr and Markus Dabernig and Achim Zeileis},
  journal = {Bulletin of the American Meteorological Society},
  year = {2009},
  volume = {96},
  number = {2},
  pages = {203--216},
  doi = {10.1175/BAMS-D-13-00155.1},
}
@Article{R-ICC,
  title = {Guidelines for Estimating Repeatability},
  author = {Matthew E. Wolak and Daphne J. Fairbairn and Yale R. Paulsen},
  year = {2012},
  journal = {Methods in Ecology and Evolution 3(1):129-137},
}
@Manual{R-tableone,
  title = {tableone: Create 'Table 1' to Describe Baseline Characteristics},
  author = {Kazuki Yoshida},
  year = {2019},
  note = {R package version 0.10.0},
  url = {https://CRAN.R-project.org/package=tableone},
}
